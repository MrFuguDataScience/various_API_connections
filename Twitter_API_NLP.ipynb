{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API using Python | Featuring `NLTK`\n",
    "\n",
    "`------------------------------------------------------------------------------`\n",
    "\n",
    "# `with Mr Fugu Data Science`\n",
    "\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "[Github](https://github.com/MrFuguDataScience) | [Youtube](https://www.youtube.com/channel/UCbni-TDI-Ub8VlGaP8HLTNw?view_as=subscriber)\n",
    "\n",
    "# Purpose & Outcome:\n",
    "\n",
    "+ Connect to Twitter API using Python\n",
    "    + Extract tweets\n",
    "+ Use NLTK, to parse each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64 # encoding\n",
    "import requests # make a connection\n",
    "import nltk # nlp\n",
    "import re # regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/basics/authentication/api-reference/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDENTIALS REPLACE WITH YOUR OWN:\n",
    "\n",
    "client_key = '' # PUT YOURS HERE\n",
    "client_secret = '' # PUT YOURS HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call your two keys: and encode \n",
    "key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')\n",
    "#use base64 to encode the keys to binary\n",
    "b64_encoded_key = base64.b64encode(key_secret)\n",
    "# convert to ascii\n",
    "b64_encoded_key = b64_encoded_key.decode('ascii')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.twitter.com/'\n",
    "\n",
    "auth_endpoint = base_url+'oauth2/token'\n",
    "\n",
    "auth_headers = { 'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "                'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'}\n",
    "\n",
    "auth_data = { 'grant_type': 'client_credentials'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post Request:\n",
    "response = requests.post(auth_endpoint, headers=auth_headers, data=auth_data)\n",
    "response.status_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n{'token_type': 'bearer',\\n 'access_token': 'your token will be here'}\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = response.json()\n",
    "# json_data\n",
    "\n",
    "'''\n",
    "{'token_type': 'bearer',\n",
    " 'access_token': 'your token will be here'}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'access_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b6fd825a9c54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccess_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'access_token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# access_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'access_token'"
     ]
    }
   ],
   "source": [
    "access_token = json_data['access_token']\n",
    "\n",
    "# access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Request:\n",
    "search_headers = {'Authorization': 'Bearer {}'.format(access_token)    \n",
    "}\n",
    "\n",
    "parameters = { 'q': 'climate change',\n",
    "                    'result_type': 'recent',\n",
    "                'count': 50 }\n",
    "\n",
    "search_url = base_url+'1.1/search/tweets.json'\n",
    "\n",
    "response = requests.get(search_url, headers=search_headers, params=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ you will see that your data is inside of statuses, then you have to go inside of text to find the actual tweet you want. There are other parameters you can extract if you need them, it just depends on what you want to evaluate later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_tweet_data = response.json()\n",
    "# json_tweet_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import csv\n",
    "# t=[]\n",
    "# with open('twitter_nest03.csv', 'w') as f:\n",
    "#     for i in json_tweet_data['statuses']:\n",
    "#         t.append(i)\n",
    "#         )\n",
    "# t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for status in json_tweet_data['statuses']:\n",
    "    print(status['text'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tolower=[]\n",
    "for status in json_tweet_data['statuses']:\n",
    "# take tweets and convert to lowercase\n",
    "    text_tolower.append(status['text'].lower())\n",
    "text_tolower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save urls if needed:\n",
    "\n",
    "save_urls=[]\n",
    "for i in text_tolower:\n",
    "    save_urls.append(re.findall(r'\\b(?:http.*)',i))\n",
    "save_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove urls from tweets:\n",
    "remove_urls=[]\n",
    "for i in text_tolower:\n",
    "    remove_urls.append(re.sub(r'\\b(?:http.*)', '', i, flags=re.MULTILINE))\n",
    "remove_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store webpages as separate list; these will have where the tweets are located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize \"split sentences of strings into lists of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-719791bc10bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mww\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mremove_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# taking all words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mww\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_urls' is not defined"
     ]
    }
   ],
   "source": [
    "ww=[]\n",
    "\n",
    "for i in remove_urls:\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\") # taking all words\n",
    "    ww.append(tokenizer.tokenize(i))\n",
    "ww[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'albomp', 'polls', 'closed', 'eden', 'monaro', 'want', 'thank', 'everyone', 'voted', 'strong', 'abc', 'properly', 'funded', 'hospitals']\n",
      "['rt', 'ecowarriorss', 'climate', 'change', 'fueling', 'summer', 'dust', 'storms', 'doubled', 'american', 'southwest', '1990s', '2000s', 'american']\n",
      "['rt', 'soiodown', 'get', 'winter', 'stage', 'white', 'outfits', 'tonight', 'say', 'blackpink', 'encouraging', 'climate', 'change']\n",
      "['rt', 'vegix', 'earth', 'ecology', 'support', 'life', 'know', 'dying', 'biggest', 'crisis', 'time', 'potential']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "for i in ww:\n",
    "    print( [w for w in i if w not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_tweet_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-34a6ccdcb61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_tweet_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statuses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# import json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json_tweet_data' is not defined"
     ]
    }
   ],
   "source": [
    "t=[]\n",
    "for i in json_tweet_data['statuses']:\n",
    "        t.append(i)\n",
    "        \n",
    "# import json\n",
    "# with open('tweets04.json', 'w') as fout:\n",
    "#     json.dump(t , fout)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`---------------------------`\n",
    "\n",
    "# <font color=red>Like</font>, Share\n",
    "\n",
    "# <font color=red>SUBscribe</font> & Turn ON Notification Bell 🔔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations & Help:\n",
    "\n",
    "https://github.com/psnegi/data_science_tools1/blob/master/notebooks/web_api.ipynb\n",
    "\n",
    "https://developer.twitter.com/en/docs/basics/authentication/api-reference/token\n",
    "\n",
    "https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets\n",
    "\n",
    "https://developer.twitter.com/en/docs/api-reference-index\n",
    "\n",
    "https://stackoverflow.com/questions/10756427/loop-through-all-nested-dictionary-values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
